#' Generates image and video representations useful in earth observation tasks for casual inference.
#'
#' Generates image and video representations useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @param file Path to a tfrecord file generated by `causalimages::WriteTfRecord`.
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize Dimensions used in the convolution kernels.
#' @param nWidth_ImageRep Number of embedding features output.
#' @param strides  Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`) Initial image processing function. Usually left `NULL`.
#' @param batchSize Integer specifying batch size in obtaining representations.
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types. Default is `"image"`.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed Integer specifying the seed for pseudo random number generation.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `Representations` (matrix) A matrix containing image/video representations, with rows corresponding to observations.
#' \item `ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch` (functions) Image modeling functions.
#' \item `ImageModel_And_State_And_MPPolicy_List` List containing image model parameters fed into functions.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

GetImageRepresentations <- function(
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = "CausalImagesEnv",
    conda_env_required = T,
    returnContents = T,
    getRepresentations = T,
    imageModelClass = "VisionTransformer",
    NORM_MEAN = NULL, 
    NORM_SD = NULL, 
    Sys.setenv_text = NULL,

    InitImageProcess = NULL,
    pretrainedModel = NULL, 
    lat = NULL, long = NULL, 
    nWidth_ImageRep = 64L,
    nDepth_ImageRep = 1L,
    nDepth_TemporalRep = 1L,
    batchSize = 16L,
    optimizeImageRep = T,
    strides = 1L,
    kernelSize = 3L,
    patchEmbedDim = 16L,
    TfRecords_BufferScaler = 10L,
    dropoutRate,
    dataType = "image",
    bn_momentum = 0.99,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    seed = NULL){

  # initialize tensorflow if not already initialized
  if(   !"logical" %in% class(try(as.numeric(np$array(jnp$square(1.)))==1,T)) ){
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    library(tensorflow); 
    if(!is.null(conda_env)){ try(reticulate::use_condaenv(conda_env, required = conda_env_required),T) }
    if(!is.null(Sys.setenv_text)){ 
      #eval(parse(text = Sys.setenv_text)) 
      eval(parse(text = Sys.setenv_text), envir = .GlobalEnv)
    }
    py_gc <- reticulate::import("gc")
    (jax <<- reticulate::import("jax"))$config$update("jax_enable_x64", FALSE);
    jnp <<- reticulate::import("jax.numpy")
    np <<- reticulate::import("numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    eq <<- reticulate::import("equinox")
    
    # set memory growth for tensorflow 
    for(device_ in tf$config$list_physical_devices()){ try(tf$config$experimental$set_memory_growth(device_, T),T) }
  }
  gc(); try(py_gc$collect(), T)
  if(is.null(seed)){ seed <- as.integer(runif(1,1,10000)) }

  # image dtype
  image_dtype <- jnp$float16
  image_dtype_tf <- tf$float16
  
  print2("Setting input types ...") 
  if(!is.null(pretrainedModel)){ pretrainedModel <- as.character(pretrainedModel) } 
  if(!is.null(optimizeImageRep)){ optimizeImageRep <- as.logical(optimizeImageRep) }
  if(!is.null(imageModelClass)){ imageModelClass <- as.character(imageModelClass) }
  if(!is.null(nWidth_ImageRep)){ nWidth_ImageRep <- as.integer(f2n(nWidth_ImageRep)) }

  if( batchSize > length( unique(imageKeysOfUnits) )){
    batchSize <- length( unique(imageKeysOfUnits)  )
  }

  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    setwd( new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/") )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  }

  # acquire image to set dimensions
  setwd(orig_wd); test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)))[[1]],0L); setwd(new_wd)
  imageDims <- ai( length( dim(test_) ) - 2L )
  rawChannelDims <- ai( dim(test_)[length(dim(test_))] )
  rawSpatialDims <- ai( dim(test_)[length(dim(test_))-1] )

  # setup jax model
  {
    print2("Setting up image representation model...")
    if(dataType == "video"){ NonLinearScaler <- jnp$array( 1/sqrt( 2*(nDepth_ImageRep + nDepth_TemporalRep )) )  };
    if(dataType == "image"){ NonLinearScaler <- jnp$array( 1/sqrt( 2*nDepth_ImageRep ) )  }
    #NonLinearScaler <- jnp$array( 1. ) 
    MPList <- list(jmp$Policy(compute_dtype="float16",  param_dtype="float32", output_dtype="float32"),
                   jmp$DynamicLossScale(jnp$array(2^15), period = 1000L))

    # coerce to integer for safety
    kernelSize <- ai(kernelSize); strides <- ai(strides)
    rawChannelDims <- ai(rawChannelDims); nWidth_ImageRep <- ai(nWidth_ImageRep)

    # set batch name
    batch_axis_name <- "batch";
    if(!"bn_momentum" %in% ls()){ bn_momentum <- 0.90 }

    # transformer preliminaries
    ffmap <- jax$vmap(function(L_, x){ L_(x) }, in_axes = list(NULL,0L))
    RMS_norm <- function(x_){ jnp$divide( x_, jnp$sqrt(0.001+jnp$mean(jnp$square(x_), -1L, keepdims=T))) }
    LayerNorm <- function(x_){ jax$nn$standardize( x_, -1L, epsilon = 1e-5) }
    InvSoftPlus <- function(x){ jnp$log(jnp$exp(x) - 1) }
    
    # Calculate number of patches
    nPatches_side = ai(rawSpatialDims / patchEmbedDim)
    nPatches = ai(nPatches_side^2)
    InitialPatchDims <- ai( (nPatches_side*patchEmbedDim*nPatches_side*patchEmbedDim)/nPatches * rawChannelDims )
    
    # adjust widths as needed 
    nWidth_VideoRep <- ifelse(optimizeImageRep,
                              yes = nWidth_ImageRep,
                              no = nWidth_ImageRep + 2L*nWidth_ImageRep*(imageModelClass=="CNN" & 
                                                                           optimizeImageRep == F ))
    if(!is.null(pretrainedModel)){ if(pretrainedModel == "vit-base"){ nWidth_ImageRep <- nWidth_VideoRep <- 768L } }  
    if(!is.null(pretrainedModel)){ if(pretrainedModel == "dino"){ nWidth_ImageRep <- nWidth_VideoRep <- 768L } }  
    if(!is.null(pretrainedModel)){ if(grepl(pretrainedModel, pattern = "videomae")){ nWidth_ImageRep <- nWidth_VideoRep <- 2L*768L } }  
    if(!is.null(pretrainedModel)){ if(pretrainedModel == "clay"){ nWidth_ImageRep <- nWidth_VideoRep <- 768L } }  
    if(!is.null(pretrainedModel)){ if(pretrainedModel == "clip-rsicd"){ nWidth_ImageRep <- nWidth_VideoRep <- 512L } }  
    
    # rotary embedding setup
    if(T == F){ 
      theta_vals_patch <-  10000^( -(2*( 1:(nWidth_ImageRep/2) )) / nWidth_ImageRep ) # p. 5
      theta_vals_patch <- jnp$expand_dims(jnp$array(unlist(sapply(theta_vals_patch,function(z){list(c(z,z))}))), 0L)
      position_patch <- jnp$expand_dims( jnp$arange(1L, (ai(nTimeSteps_patch <- ai(nPatches_side^2))) +3L), 1L)  # + 3L for stop, start
      cos_terms_patch <- jnp$cos( pos_times_theta_patch <- (position_patch *  theta_vals_patch) ) # p. 7
      sin_terms_patch <- jnp$sin( pos_times_theta_patch )
    }
    RotaryPositionalEmbeddings_spatial <- eq$nn$RotaryPositionalEmbedding( ifelse(optimizeImageRep, 
                                                                          yes = nWidth_ImageRep, 
                                                                          no = ifelse(imageModelClass == "CNN" & is.null(pretrainedModel), 
                                                                                      yes = 3L*nWidth_ImageRep, no = nWidth_ImageRep) ) ) 
    RotaryPositionalEmbeddings_temporal <- eq$nn$RotaryPositionalEmbedding( nWidth_VideoRep  ) 
    WideMultiplicationFactor <- 3.5
    nTransformerOutputWidth <- nWidth_ImageRep
    
    # set up model
    if(imageModelClass == "VisionTransformer"){
      StateList <- ModelList <- replicate(nDepth_ImageRep+1, jnp$array(0.)) # initialize with 0's
      for(d_ in 1L:nDepth_ImageRep){
          SpatialTransformerRenormer_d <- list(jnp$array( t(rep(1,times=nWidth_ImageRep) ) ),
                                               jnp$array( t(rep(1,times=nWidth_ImageRep) ) ))
          SpatialMultihead_d <- eq$nn$MultiheadAttention(
                                    query_size = nWidth_ImageRep,
                                    output_size = nWidth_ImageRep,
                                    num_heads = 12L,
                                    use_output_bias = F,
                                    key = jax$random$PRNGKey( 23453355L + seed + d_) )
          SpatialFF_d <- list(eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # hidden bias
                                           key = jax$random$PRNGKey(ai(3340L + seed + d_))),
                              eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # swiglu bias
                                           key = jax$random$PRNGKey(ai(3311L + seed + d_))),
                              eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           out_features = nWidth_ImageRep,
                                           use_bias = F, # final bias
                                           key = jax$random$PRNGKey(ai(33324L + seed +  d_))))
          StateList[[d_]] <- eval(parse(text = sprintf("list('BNState_ImRep_d%s'= jnp$array(0.))", d_)))
          ModelList[[d_]] <- eval(parse(text = sprintf('list("SpatialMultihead_d%s" = SpatialMultihead_d,
                                                            "SpatialFF_d%s" = SpatialFF_d,
                                                            "SpatialResidualWts_d%s" = list(InvSoftPlus(NonLinearScaler),InvSoftPlus(NonLinearScaler)),
                                                            "SpatialTransformerRenormer_d%s" = SpatialTransformerRenormer_d)', d_, d_, d_, d_ )))
      }
      ModelList[[d_+1]] <- list("SpatialTransformerSupp" = list(
          jax$random$uniform(key = jax$random$PRNGKey(ai(333324L + seed +  d_)), minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Start
          jax$random$uniform(key = jax$random$PRNGKey(ai(3332124L + seed +  d_)),minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Stop
          jnp$array(rep(1,times = nWidth_ImageRep)),  # RMS weighter
          eq$nn$Conv(kernel_size = c(patchEmbedDim, patchEmbedDim),
                     num_spatial_dims = 2L, stride = c(patchEmbedDim,patchEmbedDim),
                     padding_mode = "REFLECT",
                     in_channels = rawChannelDims, use_bias = F,
                     out_channels = nWidth_ImageRep, key = jax$random$PRNGKey(4L+1040L+seed)), # patch embed
          eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
                        use_bias = F, key = jax$random$PRNGKey(999L + seed  ) ) # final dense proj
        ))
    }
    TransformerBackbone <- function(ModelList, m, StateList, seed, MPList, inference, type){
      if(type == "Spatial"){ # patch embed
          m <- LE(ModelList,"SpatialTransformerSupp")[[4]](jnp$transpose(m, c(2L, 0L, 1L)))
          m <- jnp$transpose(jnp$reshape(m, list(m$shape[[1]],-1L)))
          print2(sprintf("Transformer dims: [%s]", paste(unlist(m$shape),collapse=",")))
      }

      # append start and stop condons
      m <- jnp$concatenate(list(LE(ModelList,sprintf("%sTransformerSupp",type))[[1]], m), 0L)
      # m <- jnp$concatenate(list(m, LE(ModelList,sprintf("%sTransformerSupp",type))[[2]]), 0L) # comment if no stop codon

      # start neural block
      DepthOfThisTransformer <- ifelse(type=="Spatial", yes = nDepth_ImageRep, no = nDepth_TemporalRep)
      print2(sprintf("Starting Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))
      mtm1 <- m; for(d_ in 1L:DepthOfThisTransformer){
          # standardize
          m <- RMS_norm(m)*LE(ModelList,sprintf("%sTransformerRenormer_d%s",type, d_))[[1]]

          # rotary embeddings
          if(T == F){ 
            m_pos <- MPList[[1]]$cast_to_compute( jnp$zeros_like( m ) )
            for( IDX in seq(0L, nWidth_ImageRep, by = 2L)){
              m_pos <- m_pos$at[,jnp$array(IDX)]$add(  jnp$negative(jnp$take(m, IDX+1L, axis = 1L) ) )
              m_pos <- m_pos$at[,jnp$array(IDX+1L)]$add( jnp$take(m, IDX, axis = 1L) )
            }
            m_pos <- m*jnp$take( MPList[[1]]$cast_to_compute(cos_terms_patch), jnp$array(0L:(m$shape[[1]]-1L)), 0L) +
                      m_pos*jnp$take(MPList[[1]]$cast_to_compute(sin_terms_patch), jnp$array(0L:(m$shape[[1]]-1L)), 0L) # p. 7
          }
          if(type == "Spatial"){ m_pos <- RotaryPositionalEmbeddings_spatial( m ) } 
          if(type == "Temporal"){ m_pos <- RotaryPositionalEmbeddings_temporal( m ) } 

          # multihead attention block
          m <- LE(ModelList,sprintf("%sMultihead_d%s",type,d_))(
                      query = m_pos,
                      key_  = m_pos,
                      value = m) #key = seed, # breaks in GPU mode

          # residual connection
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( LE(ModelList,sprintf("%sResidualWts_d%s",type,d_))[[1]]$astype(jnp$float32) )$astype(mtm1$dtype)

          # normalize
          m <- RMS_norm(m) * LE(ModelList,sprintf("%sTransformerRenormer_d%s",type,d_))[[2]]

          # feed forward
          m <- jax$nn$swish(ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[1]], m)) *
                        ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[2]], m) # swiglu proj
          m <- ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[3]], m) # final proj

          # residual connection to previous state
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( LE(ModelList,sprintf("%sResidualWts_d%s",type,d_))[[2]]$astype(jnp$float32) )$astype(mtm1$dtype)
      }
      print2(sprintf("Done with Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))

      # take CLS embedding from position 0 
      m <- jnp$take(m, indices = 0L, axis = 0L)

      # path control -- only executed in final pass over sequences
      if( (dataType == "image" & type == "Spatial") |  (dataType == "video" & type == "Temporal") ){
        # final norm
        # m <- jnp$squeeze(LayerNorm( jnp$expand_dims(m,0L) )*LE(ModelList,sprintf("%sTransformerSupp",type))[[3]])

        # linear proj, note: dense starts with linear projection  
        # m <- LE(ModelList,sprintf("%sTransformerSupp", type))[[5]]( m )  
      }
    return( list(m, StateList) )
    }
    if(imageModelClass == "CNN"){
    StateList <- ModelList <- replicate(nDepth_ImageRep, list())
    for(d_ in 1L:nDepth_ImageRep){
        if(d_ > 1){ strides <- 1L }
      SeperableSpatial <- eq$nn$Conv(kernel_size = c(kernelSize, kernelSize),
                                         num_spatial_dims = 2L, stride = c(strides, strides),
                                         in_channels = dimsSpatial <- ai(ifelse(d_ == 1L, yes = rawChannelDims, no = nWidth_ImageRep)),
                                         out_channels = dimsSpatial, use_bias = F,
                                         groups = dimsSpatial,
                                         key = jax$random$PRNGKey(4L+d_+seed))
      SeperableFeature <- eq$nn$Conv(in_channels = dimsSpatial, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = T,
                                         key = jax$random$PRNGKey(50L+d_+seed))
      SeperableFeature2 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                         key = jax$random$PRNGKey(530L+d_+seed))
      SeperableFeature3 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(5340L+d_+seed))
      SeperableFeature4 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(53140L+d_+seed))
      ResidualTm1Path <- eq$nn$Conv(in_channels = dimsSpatial, 
                                        out_channels = nWidth_ImageRep,
                                        groups = 1L,
                                        kernel_size = c(1L,1L),
                                        num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                        key = jax$random$PRNGKey(3250L+d_+seed))
      ResidualTPath <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                      out_channels = nWidth_ImageRep,
                                      groups = 1L,
                                      kernel_size = c(1L,1L),
                                      num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                      key = jax$random$PRNGKey(32520L+d_+seed))

        # reset weights with Xavier/Glorot
        if(T == F){ 
        SeperableSpatial <- eq$tree_at(function(l){l$weight}, SeperableSpatial,
                                           jax$random$uniform(key=jax$random$PRNGKey(5L+d_+seed),
                                                              minval = -sqrt(6/(dimsSpatial+dimsSpatial)),
                                                              maxval = sqrt(6/(dimsSpatial+dimsSpatial)),
                                                              shape = jax$tree_util$tree_leaves(SeperableSpatial)[[1]]$shape) )
        }

        # setup bn for CNN block
        LayerBN1 <- eq$nn$BatchNorm(
          #input_size = (BatchNormDim1 <- dimsSpatial), # input norm
          input_size = (BatchNormDim1 <- nWidth_ImageRep), # post-process norm
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = (BN_ep <- 0.01^2), channelwise_affine = F)
        LayerBN2 <- eq$nn$BatchNorm(
          input_size = (BatchNormDim2 <- nWidth_ImageRep), 
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)
        StateList[[d_]] <- eval(parse(text = sprintf("list('BNState_ImRep1_d%s'=list(eq$nn$State( LayerBN1 )),
                                                        'BNState_ImRep2_d%s'=list(eq$nn$State( LayerBN2 )))", d_, d_)))
        ModelList[[d_]] <- eval(parse(text = sprintf('list("SeperableSpatial_d%s" = SeperableSpatial,
                                  "SeperableFeature_d%s" = SeperableFeature,
                                  "SeperableFeature2_d%s" = SeperableFeature2, 
                                  "SeperableFeature3_d%s" = SeperableFeature3, 
                                  "SeperableFeature4_d%s" = SeperableFeature4, 
                                  "ResidualTm1Path_d%s" = ResidualTm1Path,
                                  "ResidualTPath_d%s" = ResidualTPath,
                                  "SpatialResidualWts_d%s" = list(InvSoftPlus( NonLinearScaler ),InvSoftPlus( NonLinearScaler )),
                                  "BN_ImRep1_d%s" = list(LayerBN1, jnp$array(rep(1.,times=BatchNormDim1)) ),
                                  "BN_ImRep2_d%s" = list(LayerBN2, jnp$array(rep(1.,times=BatchNormDim2)) ))', d_, d_, d_, d_, d_, d_, d_, d_, d_, d_ )))
    }
    ModelList[[d_+1]] <- list("SpatialTransformerSupp" = list(
      "FinalCNNProj"=eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nWidth_ImageRep, # final dense proj
                   use_bias = F, key = jax$random$PRNGKey(9989L + seed  ) )  ),
      "FinalCNNBN"=list(LayerBN <- eq$nn$BatchNorm(input_size = nWidth_ImageRep,
                            axis_name = batch_axis_name,
                            momentum = bn_momentum, eps = BN_ep, channelwise_affine = F), 
                            jnp$array(rep(1.,times=nWidth_ImageRep)) ))
    StateList[[d_+1]] <- eval(parse(text = "list('BNState_ImRep_FinalCNNBN'=eq$nn$State( LayerBN ))"))
    }
    if(dataType == "video"){
      for(dt_ in 1L:nDepth_TemporalRep){
        TemporalTransformerRenormer_d <- list(jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
                                              jnp$array( t(rep(1,times=nWidth_VideoRep) ) ))
        TemporalMultihead_d <- eq$nn$MultiheadAttention(
                                    query_size = nWidth_VideoRep,
                                    output_size = nWidth_VideoRep,
                                    num_heads = 8L,
                                    use_output_bias = F,
                                    key = jax$random$PRNGKey( 2343355L + seed+dt_) )
        TemporalFF_d  <- list(eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # hidden bias
                          key = jax$random$PRNGKey(ai(334300L + 1L+dt_ + seed  ))),
             eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # swiglu bias
                          key = jax$random$PRNGKey(ai(333110L + 1L+dt_ + seed ))),
             eq$nn$Linear(in_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          out_features = nWidth_VideoRep,
                          use_bias = F, # final bias
                          key = jax$random$PRNGKey(ai(3333924L + 1L + dt_ + seed ))))
        ModelList[[length(ModelList) + 1]] <- eval(parse(text = sprintf('list("TemporalTransformerRenormer_d%s" = TemporalTransformerRenormer_d,
                                                "TemporalMultihead_d%s" = TemporalMultihead_d,
                                               "TemporalResidualWts_d%s" = list(InvSoftPlus( NonLinearScaler ), InvSoftPlus( NonLinearScaler )),
                                               "TemporalFF_d%s" = TemporalFF_d)', dt_,dt_,dt_,dt_)))
      }
      ModelList[[length(ModelList)+1]] <- list("TemporalTransformerSupp" = list(
                  jax$random$uniform(key = jax$random$PRNGKey(ai(33932124L + seed +  dt_)),
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # start 
                  jax$random$uniform(key = jax$random$PRNGKey(ai(3324L + seed +  dt_)), 
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # stop
                  jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
                  jnp$array(0.), # unused  in temporal
                  eq$nn$Linear(in_features = nWidth_VideoRep, out_features =  nWidth_VideoRep,
                               use_bias = F, key = jax$random$PRNGKey(1999L+dt_+seed  ))
                  ))
    }

    # m <- InitImageProcess( jnp$array( batch_inference[[1]]),T)[0,0,,,];  d__ <- 1L; inference <- F
    # m <- InitImageProcess( jnp$array( batch_inference[[1]]), T);  d__ <- 1L; inference <- F
    ImageRepArm_SpatialArm <- function(ModelList, m, StateList, seed, MPList, inference){
      if(imageModelClass == "VisionTransformer" ){
          m <- TransformerBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
          StateList <- m[[2]]; m <- m[[1]]
      }
      if(imageModelClass == "CNN" ){
          m <- jnp$transpose(m, c(2L, 0L, 1L)) # transpose to CWH
          for(d__ in 1:nDepth_ImageRep){
            print(sprintf("CNN depth %s of %s",d__,nDepth_ImageRep))
            
            # residual path 
            mtm1 <- m
            
            # seperable spatial convolution   
            m <- LE(ModelList,sprintf("SeperableFeature_d%s",d__))(
                    LE(ModelList,sprintf("SeperableSpatial_d%s",d__))(m))

            if(! optimizeImageRep){
              #m <- jax$nn$swish( 
                #LE(ModelList,sprintf("SeperableFeature2_d%s",d__))(m) )  * LE(ModelList,sprintf("SeperableFeature3_d%s",d__))(m)
              # m <- LE(ModelList,sprintf("SeperableFeature4_d%s",d__))(m)
              # hist(np$array(m$val)) 
              
              m <- jnp$concatenate(list(mx_ <- jnp$max(m, c(1L:2L)), 
                                        mn_ <- jnp$mean(m, c(1L:2L)), 
                                        mx_ * mn_), 0L)
              return( list(m, StateList)  )
            }
            
            if( optimizeImageRep ){
              if( T == T ){  print("CNN BN (pre swiglu)")
                m <- LE(ModelList, sprintf("BN_ImRep1_d%s",d__))[[1]](m, state = LE(StateList, sprintf("BNState_ImRep1_d%s",d__))[[1]], inference = inference)
                StateIndex <- paste(sapply(LE_index( StateList, sprintf("BNState_ImRep1_d%s",d__) ),
                                           function(zer){ paste("[[", zer, "]]") }), collapse = "")
                eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
                m <- m * jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep1_d%s",d__))[[2]],1L),1L) 
              }
              
              # swiglu act fxn
              m <- jax$nn$swish(  LE(ModelList,sprintf("SeperableFeature2_d%s",d__))(m) )  *
                              LE(ModelList,sprintf("SeperableFeature3_d%s",d__))(m)
              m <- LE(ModelList,sprintf("SeperableFeature4_d%s",d__))(m)
              
              if( T == F ){  print("CNN BN (post swiglu)")
                m <- LE(ModelList, sprintf("BN_ImRep2_d%s",d__))[[1]](m, state = LE(StateList, sprintf("BNState_ImRep2_d%s",d__))[[1]], inference = inference)
                StateIndex <- paste(sapply(LE_index( StateList, sprintf("BNState_ImRep2_d%s",d__) ),
                                           function(zer){ paste("[[", zer, "]]") }), collapse = "")
                eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
                m <- m * jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep2_d%s",d__))[[2]],1L),1L) 
              }
              
              # adaptive max pooling 
              if(d__ %% 2 %in% c(0,1)){ 
                m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]*(SpatialShrinkRate <- 0.75)), ai(m$shape[[3]]*SpatialShrinkRate)))( m ) 
                m <- LE(ModelList,sprintf("ResidualTPath_d%s",d__))(m)
              }
              
              if(T == F){ 
                hist(c(np$array(mtm1$val))); apply(np$array(mtm1$val),2,sd)
                hist(c(np$array(m$val))); apply(np$array(m$val),2,sd)
                hist(c(np$array( eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))( LE(ModelList,sprintf("ResidualTm1Path_d%s",d__))(mtm1))$val)))
              }
              
              # residual connection 
              m <- eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))(
                            LE(ModelList,sprintf("ResidualTm1Path_d%s",d__))(mtm1)) +  
                         m * jax$nn$softplus( LE(ModelList,sprintf("SpatialResidualWts_d%s",d_))[[2]]$astype(jnp$float32) )$astype(mtm1$dtype)
            }
          }
          
          # final pooling, final norm (if used), and projection 
          m <- jnp$max(m, c(1L:2L))
          if(optimizeImageRep & T == T){
              print2("Performing final CNN normalization...")
              m <- LE(ModelList, "FinalCNNBN")[[1]](m, state = LE(StateList, "BNState_ImRep_FinalCNNBN"), 
                                                    inference = inference)
              StateIndex <- paste(sapply(LE_index( StateList, "BNState_ImRep_FinalCNNBN" ), function(zer){ paste("[[", zer, "]]") }), collapse = "")
              eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
              m <- m * LE(ModelList, "FinalCNNBN")[[2]] 
          }
          # m <- LE(ModelList,"FinalCNNProj")(m) # note: projection done in next dense layer
      }
      return( list(m, StateList)  )
    }
    #ImageRepArm_batch <- (ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){ print("DEBUG MODE IS ON IN IMAGE BACKBONE"); Sys.sleep(5L); 
    ImageRepArm_batch <- eq$filter_jit(ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){
      ModelList <- MPList[[1]]$cast_to_compute( ModelList )
      StateList <- MPList[[1]]$cast_to_compute( StateList )

      # squeeze temporal dim if needed
      thisPath <- T;if(!is.null(pretrainedModel)){ thisPath <- !grepl(pretrainedModel,pattern="video") }
      if(thisPath){
        if(dataType == "video" & is.null(pretrainedModel)){ m <- jnp$reshape(m, c(-1L, (orig_shape_m <- jnp$shape(m))[3:5])) }
  
        print2(sprintf("Image stack dims: [%s]", paste(unlist(m$shape),collapse=",")))
  
        # get spatial representation
        if( is.null(pretrainedModel) ){ 
          m <- jax$vmap(function(ModelList, m,
                                 StateList, seed, MPList, inference){
                ImageRepArm_SpatialArm(ModelList, m,
                                       StateList, seed, MPList, inference) },
                   in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
                   axis_name = batch_axis_name,
                   out_axes = list(0L,NULL))(ModelList, m, StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
        }
  
        # unsqueeze temporal dim if needed
        if(dataType == "video"){
          if( is.null(pretrainedModel) ){ m <- jnp$reshape(m, c(orig_shape_m[1:2], -1L)) } 
          # (np$array(m)[1:5,,sample(1:10,1)]); m$shape
          m <- jax$vmap(function(ModelList, m,
                                 StateList, seed, MPList, inference){
                      TransformerBackbone(ModelList, m,
                                          StateList, seed, MPList, inference, type = "Temporal")},
                        in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
                        axis_name = batch_axis_name,
                        out_axes = list(0L,NULL))(ModelList, m,
                                                  StateList, jnp$add(seed,42L), MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
          # plot(np$array(m)[,sample(1:10,2)]); m$shape
        }
      }
      return( list(m, StateList) )
    })
  }

  if( is.null(InitImageProcess) ){
    InitImageProcess <- (function(im, seed, inference =  F){ return( im  ) })
  }
  if( !is.null(pretrainedModel) ){
    InitImageProcess_orig <- InitImageProcess
    InitImageProcess <- function(m, seed, inference){ 
      if(!"TransformersModule" %in% ls() & !grepl(pretrainedModel,pattern="clay")){ 
        "
        conda create -n hface python=3.11 
        conda activate hface 
        python3 -m pip install --upgrade transformers torch tensorflow tensorflow_datasets pillow
        pip install -U 'jax[cuda12]'
        python3 -m pip install --upgrade jmp optax equinox 
        "
        TransformersModule <<- reticulate::import("transformers") 
        torch <<- reticulate::import("torch") 
      } 
      
      # normalize for this/these models
      if( grepl(pretrainedModel,pattern="clay")  ){ 
        m <- InitImageProcess_orig(m, jax$random$PRNGKey(1L), T) 
      }
      
      # model specific transformations 
      if(!grepl(pretrainedModel,pattern="video")){
        if( dataType == "video" ){ 
          m_shape_orig <- m$shape
          m <- jnp$reshape(m, list(-1L,m$shape[[3]],m$shape[[4]], m$shape[[5]]))
        }
        if(m$shape[[4]] == 1L){ m <- m * jnp$expand_dims(jnp$expand_dims(jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
        if(m$shape[[4]] > 3L){ m <- jnp$take(m,0L:2L,axis=3L) }
      }
      if( grepl(pretrainedModel, pattern = "clip-rsicd") ){
          # https://huggingface.co/flax-community/clip-rsicd-v2
            if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
              print("Loading pre-trained model (clip-rsicd)...")
              
              PretrainedImageModelName <<- "flax-community/clip-rsicd-v2"
              FeatureExtractor <<- TransformersModule$CLIPProcessor$from_pretrained(PretrainedImageModelName)
              torch$set_default_device(
                RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                       yes = list(torch$device("cuda")), 
                                       no = list(torch$device("cpu")))[[1]] 
              )
              torch$set_default_dtype( RunDtype <<- torch$float32 ); 
              TransformersModel <<- TransformersModule$CLIPModel$from_pretrained(PretrainedImageModelName)$to(RunOnDevice)#$half()
              nParameters_Pretrained <<- TransformersModel$num_parameters()
              
              # from examining FeatureExtractor (?)
              MEAN_RESCALER <<- jnp$array(c(0, 0,0))
              MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,3L,1L,1L))
              
              SD_RESCALER <<- jnp$array(c(1,1,1))
              SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,3L,1L,1L))
              
              NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
              NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,3L))
            }
        

            # stretch and re-normalize. see FeatureExtractor for details 
            m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
            m <- jax$image$resize(
              image=m,
              shape=c(m$shape[[1]], 224L, 224L, 3L),
              method="bilinear")
            m <- jnp$transpose(m, c(0L,3L,1L,2L))
            m <- (m*SD_RESCALER)+MEAN_RESCALER
            # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
            m <- torch$tensor( reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32), dtype = torch$float32)
            # m <- reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32)
            # m <- FeatureExtractor(images = m, return_tensors="pt", do_resize = T)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
            m <- TransformersModel$get_image_features(pixel_values = m)$cpu()$detach()$numpy() 
            py_gc$collect()
      }
      if( grepl(pretrainedModel, pattern = "vit-base") ){ 
          if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
            print("Loading pre-trained model (vit-base)...")
            PretrainedImageModelName <<- 'google/vit-base-patch16-224-in21k'
            FeatureExtractor <<- TransformersModule$ViTImageProcessor$from_pretrained(PretrainedImageModelName)
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            TransformersModel <<- TransformersModule$ViTModel$from_pretrained(PretrainedImageModelName)$to(RunOnDevice)#$half()
            nParameters_Pretrained <<- TransformersModel$num_parameters()
            #TransformersModel <- torch$compile(TransformersModel)
            
            MEAN_RESCALER <<- jnp$array(c(0.5,0.5,0.5))
            MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,3L,1L,1L))
            
            SD_RESCALER <<- jnp$array(c(0.5,0.5,0.5))
            SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,3L,1L,1L))
            
            NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
            NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,3L))
          }

          # Images are resized/rescaled to the same resolution (224x224)
          # and normalized across the RGB channels with mean (0.5, 0.5, 0.5) 
          # and standard deviation (0.5, 0.5, 0.5)
          # evidence: https://huggingface.co/google/vit-base-patch16-224-in21k  
          # (see preprocessing)
          # m_orig <- m
          # jnp$mean(jnp$array(m), axis = c(0L:2L)); jnp$std(jnp$array(m), axis =  c(0L:2L)) 
          m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
          
          # resize information: 
          #the output shape, as a sequence of integers with length equal to
          #the number of dimensions of `image`. Note that :func:`resize` does not
          #distinguish spatial dimensions from batch or channel dimensions, so this
          #includes all dimensions of the image. To represent a batch or a channel
          #dimension, simply leave that element of the shape unchanged.
          
          # sanity checks 
          #image2(np$array(m[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear")[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 64L, 64L, 3L), method="bilinear")[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 32L, 32L, 3L), method="bilinear")[1,,,1]))
          
          # run resizing 
          m <- jax$image$resize(
            image=m,
            shape=c(m$shape[[1]], 224L, 224L, 3L),
            method="bilinear")
          
          #m <- FeatureExtractor(images = m, return_tensors="pt",do_resize = T, do_rescale = F, do_normalize = F)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- jnp$transpose(m, c(0L,3L,1L,2L))
          m <- (m*SD_RESCALER)+MEAN_RESCALER
          # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
          m <- torch$tensor( reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32), dtype = torch$float32)
          m <- TransformersModel(m)$pooler_output$cpu()$detach()$numpy() 
          py_gc$collect()
        }
      if( grepl(pretrainedModel, pattern = "clay") ){ 
          # https://clay-foundation.github.io/model/tutorials/clay-v1-wall-to-wall.html
          #Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
          if(!"ClayModel" %in% ls(.GlobalEnv)){
            print("Loading pre-trained model (Clay)...")
            # recipe 
            # install git-lfs
            # https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md
            # clone model wts 
            "
            cd ~/Documents 
            git clone https://huggingface.co/made-with-clay/Clay
            
            cd ~/Documents
            git clone https://github.com/Clay-foundation/model
            cd model

            # on mac: 
            conda create --name claymodel python=3.11 conda-lock=2.5.6
            conda activate claymodel
            
            conda-lock install --name claymodel conda-lock.yml

            python3 -m pip install tensorflow tensorflow_datasets 

            pip install -U 'jax[cuda12]'
            python3 -m pip install --upgrade jmp optax equinox 
            python3 -m pip install --upgrade torch torchvision
            "
            # then find ld env via 
            # sudo find / -name 'libmkl_intel_lp64.so*'
            # add lib folders to LD paths in sysenv call
            
            torch <<- import("torch")
            if( !Sys.info()["machine"] == "x86_64" ){ 
              oldwd <- getwd(); setwd("~/Documents/model/");
              ClayModel <<- reticulate::import_from_path("model", path = "./src")
              setwd(oldwd)
              ClayModel <<- ClayModel$ClayMAEModule$load_from_checkpoint(
                "/Users/cjerzak/Documents/Clay/Clay-1.0.5.7_epoch-13_val-loss-0.3098.ckpt", 
                metadata_path="/Users/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0, batch_first = T)
            }
            if( Sys.info()["machine"] == "x86_64" & T == F ){ 
              ClayModel <<- ClayModel$ClayMAEModule$load_from_checkpoint(
                "/home/cjerzak/Documents/Clay/clay-v1-base.ckpt", 
                metadata_path="/home/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0) #$torch_dtype=torch$float16)
            }
            #})
          }
          if(!"RunOnDevice" %in% ls(.GlobalEnv)){
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            #torch$set_default_tensor_type( torch$HalfTensor )
            ClayModel <<- ClayModel$to(RunOnDevice)
            
            nParameters_Pretrained <<- reticulate::as_iterator(  ClayModel$model$encoder$parameters() )
            nParameters_Pretrained_ <- 0; 
            DoneLoop <- F; while(!DoneLoop){ 
              theNextN <- try(reticulate::iter_next( nParameters_Pretrained )$numel(), T)
              if('integer' %in% class(theNextN)){ nParameters_Pretrained_ <- nParameters_Pretrained_ + theNextN  }
              if(!'integer' %in% class(theNextN)){ DoneLoop <- T }
            }
            nParameters_Pretrained <<- nParameters_Pretrained_
          }
          
          # get place embeddings 
          latlong_embed <- cbind(lat[batch_indices] * pi / 180, long[batch_indices] * pi / 180)
          latlong_embed <-  cbind(sin(latlong_embed[,1]),cos(latlong_embed[,1]),
                                  sin(latlong_embed[,2]),cos(latlong_embed[,2]))
          if(dataType == "video"){
            latlong_embed <- do.call(rbind, unlist(apply(latlong_embed,1,function(zer){
              list(cbind(rep(zer[1], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                    rep(zer[2], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                    rep(zer[3], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                    rep(zer[4], times = np$array(m$shape)[1]/m_shape_orig[[1]])))
            }), recursive = F))
            #latlong_embed <- np$array( jnp$reshape(jnp$concatenate( list(jnp$expand_dims(jnp$array(latlong_embed),1L), jnp$expand_dims(jnp$array(latlong_embed),1L)), 1L), list(-1L,4L) )) 
          }
          time_embed <- t(replicate(np$array(m$shape)[1],{c(-0.1205, -0.9927,  0.2588, -0.9659)})) 

          # obtain embeddings 
          # ClayModel$model$metadata$`landsat-c2l1`
          # RGB means: 10678.0, 10563.0, 11083.0
          # RGB stds: 9578.0, 9408.0, 10144.0
          # ClayModel$model$metadata$`landsat-c2l2-sr`
          m <- ClayModel$model$encoder(
                                        dict("platform" = "landsat-c2l1",  # platform
                                             "time" = torch$tensor( time_embed, dtype = RunDtype)$to(RunOnDevice), # temporal embedding 
                                             "latlon" = torch$tensor( latlong_embed, dtype = RunDtype )$to(RunOnDevice), # lat long embedding 
                                             #"pixels" = torch$tensor( reticulate::np_array(m$transpose(c(0L,3L,1L,2L))), dtype = RunDtype )$to(RunOnDevice), # normalized image 
                                             "pixels" = torch$tensor( reticulate::np_array(tf$constant(m$transpose(c(0L,3L,1L,2L)))), dtype = torch$float32),
                                             "gsd" = torch$tensor(30, dtype = RunDtype)$to(RunOnDevice),  # resolution 
                                             'waves' = torch$tensor(c(0.65, 0.56, 0.48), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes RGB
                                             #'waves' = torch$tensor(c(0.493, 0.560, 0.665), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes BGR
                )
          )[[1]]  
          # The first embedding is the [CLS], which is a global embedding
          m = jnp$array(  m$cpu()$detach()$numpy()[,1,] ) 
          # plot(np$array(m)[,sample(1:10,2)])
        }
      if( !grepl(pretrainedModel,pattern="video") & dataType == "video" ){ 
        # reshape if not using videomae
        m <- jnp$reshape(jnp$array(m), list(m_shape_orig[[1]], m_shape_orig[[2]], -1L) ) 
      } 
      if(grepl(pretrainedModel,pattern="videomae")){ 
        if(!"FeatureExtractor" %in% ls(.GlobalEnv) ){  # https://huggingface.co/docs/transformers/en/model_doc/videomae
          print("Loading pre-trained model (videomae)...")
          PretrainedVideoModelName <<- "MCG-NJU/videomae-base"
          #videoModelName <<- "MCG-NJU/videomae-base-finetuned-kinetics"
          
          # set device and dtypes
          torch$set_default_device(
            RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                   yes = list(torch$device("cuda")), 
                                   no = list(torch$device("cpu")))[[1]] 
          )
          torch$set_default_dtype( RunDtype <<- torch$float32 ); 

          # load models
          py_gc$collect()
          # use VideoMAEImageProcessor? 
          FeatureExtractor <<- TransformersModule$ViTImageProcessor$from_pretrained(PretrainedVideoModelName)
          TransformersModel <<- TransformersModule$ViTModel$from_pretrained(PretrainedVideoModelName, 
                                                                            torch_dtype = torch$float16)#half()$to(RunOnDevice)
          nParameters_Pretrained <<- TransformersModel$num_parameters()
          
          MEAN_RESCALER <<- jnp$array(c(1,1,1))
          MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,1L,1L,1L,3L))
          
          SD_RESCALER <<- jnp$array(c(1,1,1))
          SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,1L,1L,1L,3L))
          
          NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,1L,3L))
          NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,1L, 3L))
        }
        #m <- reticulate::np_array( tf$constant(m), dtype = np$uint8)
        
        m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
        m <- jax$image$resize(
          image=m,
          shape=c(m$shape[[1]], m$shape[[2]],  224L, 224L, 3L),
          method="bilinear")
        #m <- FeatureExtractor(images = m, return_tensors="pt",do_resize = T, do_rescale = F, do_normalize = F)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
        m <- (m*SD_RESCALER)+MEAN_RESCALER
        # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
        
        #m <- jnp$array(m, dtype = np$uint8)
        m_rep <- c(); for(j_ in 1L:as.integer(m$shape[[1]]) ){ # iterate over the batch dimension? 
          m_ <- jnp$take(m, j_-1L, axis = 0L)
          if(m_$shape[[4]] == 1L){ m_ <- m_ * jnp$expand_dims(jnp$expand_dims(jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
          if(m_$shape[[4]] > 3L){ m_ <- jnp$take(m,0L:2L,axis=3L) }
          m_ <- jnp$transpose(  m_, c(0L,3L,1L,2L))
          m_ <- torch$tensor( reticulate::np_array( tf$constant(m_, tf$float32), dtype = np$float32), dtype = torch$float32)
          #m_ <- reticulate::np_array( tf$constant(m_), dtype = np$uint8)
          
          # run model
          #m_ <- FeatureExtractor(images = m_,  do_rescale = F,  return_tensors="pt", do_resize = T)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          # output of extractor is T by C by W by H
          m_ <- TransformersModel(m_)$pooler_output$cpu()$detach()$numpy()

          # save final data 
          m_rep <- rbind( m_rep, c(colMeans(m_), apply(m_, 2,sd) ))
        }
        m <- jnp$array( m_rep )
      }
      return( m ) 
    }
  }
  
  Representations <- NULL; if(getRepresentations){
  Representations <- matrix(NA,nrow = length(unique(imageKeysOfUnits)), 
                            ncol = ifelse(optimizeImageRep, yes = nWidth_ImageRep, 
                                                            no = nWidth_ImageRep+2L*nWidth_ImageRep*(imageModelClass=="CNN")*is.null(pretrainedModel) ))
  usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
      last_i <- batch_indices[ length(batch_indices) ]

      # checks for last / batch size corrections
      if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
      batchSizeOneCorrection <- F; 
      if(length(batch_indices) == 1){ batchSizeOneCorrection <- T }

      # get the data
      setwd(orig_wd); batch_inference <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
                                                            filename = file,
                                                            nObs = length(unique(imageKeysOfUnits)),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype_tf,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]] ); setwd(new_wd)
      if(batchSizeOneCorrection){
          batch_indices <- c(batch_indices,batch_indices)
          batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                      tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
      }
      saved_iterator <- batch_inference[[2]]
      batch_inference <- batch_inference[[1]]
      batch_keys <- unlist(  lapply( p2l(batch_inference[[3]]$numpy()), as.character) )

      gc(); py_gc$collect() # collect memory
      # im <- jnp$array(batch_inference[[1]]); seed <- jax$random$PRNGKey(ai(2L+ok_counter + seed)); inference = T
      # plot( np$array( jnp$array(batch_inference[[1]]))[,,1,3])# check variability across units
      # batch_inference[[1]][3,,,1] # check image inputs 
      representation_ <-  np$array( ImageRepArm_batch(ModelList,
                                                          InitImageProcess(jnp$array(batch_inference[[1]]),
                                                                           jax$random$PRNGKey(ai(2L+ok_counter + seed)), inference = T),
                                                          StateList,
                                                          jax$random$PRNGKey(ai(last_i + seed)),
                                                          MPList, 
                                                          T # inference 
                                                          )[[1]]  )
      # plot(representation_[,sample(1:20)])
      # hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
      # plot(representation_[1,],np$array(LE(ModelList,"SpatialTransformerSupp")[[1]]))
      # plot(representation_[sample(1:4,1),],np$array(LE(ModelList,"TemporalTransformerSupp")[[1]]))
      if(T == F){ 
        # sanity checks
        batch_inference[[1]]$shape
        tmp <- np$array(jnp$take(jnp$array(batch_inference[[1]]),(iCheck <- 8L)-1L,axis=0L))
        image2(tmp[,,1])
        image2(tmp[1,,,1])
        cbind(lat[which(imageKeysOfUnits %in% batch_keys[ iCheck ])], long[which(imageKeysOfUnits %in% batch_keys[ iCheck ])])
        # dim( tmp  ) 
      }
      if(any(is.na(representation_))){stop("Stopping due to missingness in intermediary representation_ [Code reference: ImageModelBackbone.R]")}
      if("try-error" %in% class(representation_)){ print2("Error Statement:");print(representation_); browser() }
      
      if(batchSizeOneCorrection){ representation_ <- representation_[1,] }
      usedImageKeys <- c(usedImageKeys, batch_keys)
      Representations[batch_indices,] <- representation_
      print2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
  }
  Representations <- Representations[match(imageKeysOfUnits,usedImageKeys),]
  print2(sprintf("Done getting image/video representations!"))
  }

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )

  ImageModel_And_State_And_MPPolicy_List = list(ModelList, StateList, MPList);
  
  print2("Obtaining approximate parameter count...")
  nParamsRep <- sum(unlist(lapply(jax$tree_leaves(eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size})))
  if(is.null(pretrainedModel) & optimizeImageRep == F){ nParamsRep <- nParamsRep }
  if(!is.null(pretrainedModel) ){
    if(dataType == "image"){ nParamsRep <- nParameters_Pretrained }
    if(dataType == "video" & !grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParamsRep + nParameters_Pretrained }
    if(dataType == "video" & grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParameters_Pretrained }
  }

  rm(ModelList, StateList, MPList); gc()
  
  print2(getwd())
  
  # sanity check 
  if(any(is.na(Representations))){stop("Stopping due to missingness in output Representations [Code reference: ImageModelBackbone.R]")}
  
  if(returnContents){
   return( list( "ImageRepresentations"= Representations,
                 "ImageRepArm_batch_R" = ImageRepArm_batch_R,
                 "ImageRepArm_batch" = ImageRepArm_batch,
                 "ImageModel_And_State_And_MPPolicy_List" = ImageModel_And_State_And_MPPolicy_List,
                 "InitImageProcess" = InitImageProcess,
                 "nParamsRep" = nParamsRep
                 ) )
  }
}
